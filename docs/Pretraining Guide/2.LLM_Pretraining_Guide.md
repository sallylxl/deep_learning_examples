---
layout: default
title: Large Language Models
nav_order: 2
---

# Large Language Models
## Prerequisites
### [deep_learning_examples repo](https://github.com/sallylxl/deep_learning_examples) 
Throughout the rest of this document, referenced files may be found in [deep_learning_example](https://github.com/sallylxl/deep_learning_examples) repo.

#### Key Folders

- [NeMo LLM Pretraining Scripts](https://github.com/sallylxl/deep_learning_examples/tree/master/training/nemo/llm)
: Contains example scripts for pretraining LLM Models using the [NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/).  These scripts are adapted from [NeMo-Framework-Launcher](https://github.com/NVIDIA/NeMo-Framework-Launcher/tree/main)
- [Megatron-LM LLM Pretraining Scripts](https://github.com/sallylxl/deep_learning_examples/tree/master/training/Megatron-LM/llm/gpt3)
: Contains example scripts for pretraining LLM Models that adapted from [Megatron-LM](https://github.com/NVIDIA/Megatron-LM).
- [Megatron-DeepSpeed LLM Pretraining Scripts](https://github.com/sallylxl/deep_learning_examples/tree/master/training/Megatron-DeepSpeed/llm/gpt3)
: Contains example scripts for pretraining LLM Models that adapted from [Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed)
- [Kubernetes Launcher Scripts](https://github.com/sallylxl/deep_learning_examples/tree/master/launcher_scripts/k8s/training)
: Includes [Pytorchjob](https://github.com/kubeflow/pytorch-operator) YAML files for Kubernetes resources and launcher scripts. Key specs of Pytorchjob YAML include:

  + Within this resource definition, there are two important `specs`: the `master` and the `worker`. the `master` and `worker` containers all run the same script using the same arguments. Both containers take the resources of an entire node, which includes 8 GPUs.
  + The PyTorch Job will also set up all the environment variables that are needed by `torchrun` and `dist` to set up distributed training, excluding `NODE_RANK`, which should be set using `RANK`. `MASTER_ADDR` and `MASTER_PORT` will point at the pod defined by the master spec.


### Dataset Preparation

#### Synthetic Data

For testing performance, use synthetic data for pretrainning based NeMo and Megatron-LM

#### Download and Pre-process Training Dataset
To use read dataset, before executing the steps below, you can download and pre-process the training set using the following commands (see [here](https://github.com/bigscience-workshop/Megatron-DeepSpeed?tab=readme-ov-file#quick-pre-processing-to-start-training-with) for more details):

```bash
wget https://huggingface.co/bigscience/misc-test-data/resolve/main/stas/oscar-1GB.jsonl.xz
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt
xz -d oscar-1GB.jsonl.xz
python tools/preprocess_data.py \
    --input oscar-1GB.jsonl \
    --output-prefix my-gpt2 \
    --vocab-file gpt2-vocab.json \
    --dataset-impl mmap \
    --tokenizer-type GPT2BPETokenizer \
    --merge-file gpt2-merges.txt \
    --append-eod \
    --workers 8
```

### Docker Image
Use either:
- ScitiX NeMo container: `registry-ap-southeast.scitix.ai/hpc/nemo:24.07`
- the NGC NeMo container: `nemo:24.07`. If using NGC, clone this repository into the container or a shared storage accessible by distributed worker containers

## Llama Pretraining Guide

### NeMo Llama Pretraining Guide

#### Llama Pretraining Scripts

1. Llama Pretraining Python Script

    The Llama pretraining python script is based Megatron-Core and adapted from the NeMo library [megatron_Llama_pretraining.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_pretraining.py). It is available at container path: 
    `/opt/NeMo/examples/nlp/language_modeling/megatron_Llama_pretraining.py`

2. Llama Pretraining Model Configurations

    Recommended configuration for NVIDIA H100 GPUs using bf16 data type is available at (for `Llama2_13b_bf16` model):
    [`${DEEP_LEARNING_EXAMPLES_DIR}/training/llm/nemo/llm/llama2_13b_bf16/llama2_13b_bf16_hydra.yaml`](https://github.com/sallylxl/deep_learning_examples/blob/master/training/nemo/llm/llama2_13b_bf16/llama2_13b_bf16_hydra.yaml).

3. Llama Pretraining Shell Script

    The Llama pretraining shell script runs the above python script with following training parameters default (for `Llama2_13b_bf16` model):

    + The number of gradient accumulation microsteps is 128, with micro batch size of 1.
    + The tensor parallelism degree is 2.
    + The pipeline parallel degree is 1.

    The running script is available at: [`${DEEP_LEARNING_EXAMPLES_DIR}/training/llm/nemo/llm/llama2_13b_bf16/run_nemo_Llama2_13b_bf16.sh`](https://github.com/sallylxl/deep_learning_examples/blob/master/training/nemo/llm/llama2_13b_bf16/run_nemo_llama2_13b_bf16.sh)


#### Launch Job Using PyTorchjob Operator

##### K8S Job Lancher Shell Script
The Llama training Lancher shell script runs the above **Llama Pretraining Shell Script** with following training parameters default (for `Llama2_13b_bf16` model) by launching a pytorchjob:

+ The number of gradient accumulation microsteps is 128, with micro batch size of 1.
+ The tensor parallelism degree is 4.
+ The pipeline parallel degree is 1.
+ The number of workers is 1, each worker takes the resources of an entire node, which includes 8 GPUs.

The launch script `launch_nemo_llama2_13b_bf16.sh` is available at: [${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/nemo/llm/launch_nemo_llama2_13b_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/nemo/llm/launch_nemo_llama2_13b_bf16.sh).


##### Step-by-Step Guide

- **Update Variables**
: Edit the example running script, taking `Llama2_13b_bf16` pretraining for example. Modify the following variables in [lancher_nemo_llama2_13b_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/nemo/llm/launch_nemo_llama2_13b_bf16.sh):
   + `DEEP_LEARNING_EXAMPLES_DIR`: Path to this repository (default: `/workspace/deep_learning_examples`).
   + `BASE_RESULT_DIR`: Directory for experiment results (default: `$DEEP_LEARNING_EXAMPLES_DIR/results`).
   + `GPU_NUMS`: Number of GPUs used for distributed training.

- **Lanuch Command**
: 

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/nemo/llm
    ./launch_nemo_llama2_13b_bf16.sh
    ```

- **Scaling Up**
: If you want to scale up the number of GPUs used for distributed training, all you would need to do is set the number of `GPU_NUMS`，for example：

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/nemo/llm
    GPU_NUMS=16 ./launch_nemo_llama2_13b_bf16.sh
    ```
- **Monitor Training**
: You can monitor the job's progress by checking the logs of **the master pod** or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/llama2_13b_bf16/tp2-pp1-${run_id}/log-llama2_13b_bf16.log` in **master pod**. The outputs like below some lines showing throughput and loss statistics every log step. The outputs like below some lines showing throughput and loss statistics every log step.

    ```plain
    Epoch 0: :   2%|▏         | 2/128 [00:47<49:46, reduced_train_loss=10.60, global_step=1.0, consumed_samples=256, train_step_timing in s=23.7]
    ```

### Megatron-LM Llama Pretraining Guide

#### Llama Pretraining Scripts

1. Llama Pretraining Python Script

    The Llama pretraining python script is adapted from the Megatron-LM library [pretrain_gpt.py](https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_gpt.py). 

    The script is available at container path `$DEEP_LEARNING_EXAMPLES_DIR/thirdparty/Megatron-LM/pretrain_gpt.py`

2. Llama Pretraining Shell Script

    The Llama pretraining shell script runs the above python script with following training parameters default (for `Llama2_13b_bf16` model):

    + The number of gradient accumulation microsteps is 128, with micro batch size of 1.
    + The tensor parallelism degree is 2.
    + The pipeline parallel degree is 1.

    The running script is available at: [`${DEEP_LEARNING_EXAMPLES_DIR}/training/llm/Megatron-LM/llm/llama/run_meg_lm_llama2_13b_bf16.sh`](https://github.com/sallylxl/deep_learning_examples/blob/master/training/Megatron-LM/llm/llama/run_meg_lm_llama2_13b_bf16.sh)


#### Launch Job Using PyTorchjob Operator

##### K8S Job Lancher Shell Script
The Llama training Lancher shell script runs the above **Llama Pretraining Shell Script** with following training parameters default (for `Llama2_13b_bf16` model) by launching a pytorchjob:

+ The number of gradient accumulation microsteps is 128, with micro batch size of 1.
+ The tensor parallelism degree is 2.
+ The pipeline parallel degree is 1.
+ The number of workers is 2, each worker takes the resources of an entire node, which includes 8 GPUs.

The launch script `launch_meg_lm_llama2_13b_bf16.sh` is available at: [${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-lm/llm/launch_megatron_lm_llama2_13b_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/megatron-lm/llm/launch_meg_lm_llama2_13b_bf16.sh).

##### Step-by-Step Guide

- **Update Variables**
: Edit the example running script, taking `Llama2_13b_bf16` pretraining for example. Modify the following variables in [lancher_meg_lm_llama2_13b_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/megatron-lm/llm/launch_meg_lm_llama2_13b_bf16.sh):
   + `DEEP_LEARNING_EXAMPLES_DIR`: Path to this repository (default: `/workspace/deep_learning_examples`).
   + `BASE_RESULT_DIR`: Directory for experiment results (default: `$DEEP_LEARNING_EXAMPLES_DIR/results`).
   + `GPU_NUMS`: Number of GPUs used for distributed training.

- **Lanuch Command**
: 

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-lm/llm
    ./launch_meg_lm_llama2_13b_bf16.sh
    ```

- **Scaling Up**
: If you want to scale up the number of GPUs used for distributed training, all you would need to do is set the number of `GPU_NUMS`，for example：

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-lm/llm
    GPU_NUMS=32 ./launch_meg_lm_llama2_13b_bf16.sh
    ```
- **Monitor Training**
: You can monitor the job's progress by checking the logs of the **the last RANK pod** or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/meg-lm-llama2_13b_bf16/tp2-pp1-${run_id}/log-meg-lm-llama2_13b_bf16.log` in **the last RANK pod**. The outputs like below some lines showing throughput and loss statistics every log step.

    ```plain
    [2024-09-24 06:47:12] iteration       50/     128 | consumed samples:        12800 | elapsed time per iteration (ms): 12650.4 | throughput per GPU (TFLOP/s/GPU): 444.2 | learning rate: 3.488372E-05 | global batch size:   256 | lm loss: 7.581806E-02 | loss scale: 1.0 | grad norm: 2.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
    ```

### Megatron-DeepSpeed Llama Pretraining Guide

#### Llama Pretraining Scripts

1. Llama Pretraining Python Script

    The Llama pretraining python script is adapted from the Megatron-DeepSpeed library [pretrain_gpt.py](https://github.com/microsoft/Megatron-DeepSpeed/blob/main/pretrain_gpt.py). 

    The script is available at container path `$DEEP_LEARNING_EXAMPLES_DIR/thirdparty/Megatron-DeepSpeed/pretrain_gpt.py`

2. Llama Pretraining Shell Script

    The Llama pretraining shell script runs the above python script with following training parameters default (for `Llama2_13b_bf16` model):

    + The number of gradient accumulation microsteps is 128, with micro batch size of 1.
    + The tensor parallelism degree is 2.
    + The pipeline parallel degree is 1.

    The running script is available at: [`${DEEP_LEARNING_EXAMPLES_DIR}/training/llm/Megatron-DeepSpeed/llm/llama/run_ds_llama2_13b_bf16.sh`](https://github.com/sallylxl/deep_learning_examples/blob/master/training/Megatron-DeepSpeed/llm/llama/run_ds_llama2_13b_bf16.sh)


#### Launch Job Using PyTorchjob Operator

##### K8S Job Lancher Shell Script
The Llama training Lancher shell script runs the above **Llama Pretraining Shell Script** with following training parameters default (for `Llama2_13b_bf16` model) by launching a pytorchjob:

+ The number of gradient accumulation microsteps is 128, with micro batch size of 1.
+ The tensor parallelism degree is 2.
+ The pipeline parallel degree is 1.
+ The Zero-Stage is 2.
+ The number of workers is 1, each worker takes the resources of an entire node, which includes 8 GPUs.

The launch script `launch_ds_llama2_13b_bf16.sh` is available at: [${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-deepspeed/llm/launch_ds_llama2_13b_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/megatron-deepspeed/llm/launch_ds_llama2_13b_bf16.sh).

##### Step-by-Step Guide

- **Update Variables**
: Edit the example running script, taking `Llama2_13b_bf16` pretraining for example. Modify the following variables in [lancher_ds_llama2_13b_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/megatron-deepspeed/llm/launch_ds_llama2_13b_bf16.sh):
   + `DEEP_LEARNING_EXAMPLES_DIR`: Path to this repository (default: `/workspace/deep_learning_examples`).
   + `BASE_RESULT_DIR`: Directory for experiment results (default: `$DEEP_LEARNING_EXAMPLES_DIR/results`).
   + `DATA_DIR`: Path to dataset (default: `/datasets/preset/bigscience/oscar-en`)
   + `GPU_NUMS`: Number of GPUs used for distributed training.

- **Lanuch Command**
: 

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-deepspeed/llm
    ./launch_ds_llama2_13b_bf16.sh
    ```

- **Scaling Up**
: If you want to scale up the number of GPUs used for distributed training, all you would need to do is set the number of `GPU_NUMS`，for example：

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-deepspeed/llm
    GPU_NUMS=16 ./launch_ds_llama2_13b_bf16.sh
    ```
- **Monitor Training**
: You can monitor the job's progress by checking the logs of **the last RANK pod** or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/ds-llama2_13b_bf16/tp2-pp1-${run_id}/log-ds-llama2_13b_bf16.log` in **the last RANK pod**. The outputs like below some lines showing throughput and loss statistics every log step.

    ```plain
    iteration       20/     128 | consumed samples:         2560 | consumed tokens:     10485760 | elapsed time per iteration (ms): 19811.1 | learning rate: 1.395E-05 | global batch size:   128 | lm loss: 8.791158E+00 | loss scale: 1.0 | actual seqlen:  4096 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 6.461 | tokens per gpu per second (tgs): 3308.041 | TFLOPs: 383.88 |
    ```


## GPT Pretraining Guide

### NeMo GPT Pretraining Guide
#### GPT Pretraining Scripts
##### GPT Pretraining Python Script
The GPT pretraining python script is based Megatron-Core and adapted from the NeMo library [megatron_gpt_pretraining.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_pretraining.py). 

The script is available at container path `/opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py`

##### GPT Model Configurations

Recommended configuration for NVIDIA H100 GPUs using bf16 data type is available at (for `gpt3_5b_2k_bf16` model):
[`${DEEP_LEARNING_EXAMPLES_DIR}/training/llm/nemo/llm/gpt3_5b_2k_bf16/gpt3_5b_2k_bf16_hydra.yaml`](https://github.com/sallylxl/deep_learning_examples/blob/master/training/nemo/llm/gpt3_5b_2k_bf16/gpt3_5b_2k_bf16_hydra.yaml).


##### GPT Pretraining Shell Script
The GPT training Lancher shell script runs the above python script with following training  parameters default (for `gpt3_5b_2k_bf16` model):

+ The number of gradient accumulation microsteps is 128, with micro batch size of 1.
+ The tensor parallelism degree is 1.
+ The pipeline parallel degree is 1.

The script is available at [`${DEEP_LEARNING_EXAMPLES_DIR}/training/llm/nemo/llm/gpt3_5b_2k_bf16/run_nemo_gpt3_5b_2k_bf16.sh`](https://github.com/sallylxl/deep_learning_examples/blob/master/training/nemo/llm/gpt3_5b_2k_bf16/run_nemo_gpt3_5b_2k_bf16.sh)

#### Launch Job Using PyTorchjob Operator

##### K8S Job Lancher Shell Script

The GPT training Lancher shell script runs the above **GPT Pretraining Shell Script** with following training parameters default (for `gpt3_5b_2k_bf16` model)  by lanching pytorchjob::

+ The number of gradient accumulation microsteps is 128, with micro batch size of 1.
+ The tensor parallelism degree is 1.
+ The pipeline parallel degree is 1.
+ The number of workers is 1, each worker takes the resources of an entire node, which includes 8 GPUs.

The launch script `launch_nemo_gpt3_5b_2k_bf16.sh` is available at [deep_learning_examples/launcher_scripts/k8s/training/nemo/llm/launch_nemo_gpt3_5b_2k_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/nemo/llm/launch_nemo_gpt3_5b_2k_bf16.sh).
The `PyTorchJob` Kubernetes resource is defined in [launcher_scripts/k8s/training/nemo/llm/pytorchjob-nemo.yaml.template](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/nemo/llm/pytorchjob-nemo.yaml.template).


##### Step-by-Step Guide

- **Update Variables**
: Edit the example running script, taking `gpt3_5b_2k_bf16` pretraining for example. Modify the following variables in [lancher_nemo_gpt3_5b_2k_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/nemo/llm/launch_nemo_gpt3_5b_2k_bf16.sh):
   + `DEEP_LEARNING_EXAMPLES_DIR`: Path to this repository (default: `/workspace/deep_learning_examples`).
   + `BASE_RESULT_DIR`: Directory for experiment results (default: `$DEEP_LEARNING_EXAMPLES_DIR/results`).
   + `GPU_NUMS`: Number of GPUs used for distributed training.

- **Lanuch Command**
: 

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/nemo/llm
    ./launch_nemo_gpt3_5b_2k_bf16.sh
    ```

- **Scaling Up**
: If you want to scale up the number of GPUs used for distributed training, all you would need to do is set the number of `GPU_NUMS`，for example：

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/nemo/llm
    GPU_NUMS=16 ./launch_nemo_gpt3_5b_2k_bf16.sh
    ```
- **Monitor Training**
: You can monitor the job's progress by checking the logs of **the master pod** or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/gpt3-5b-2k-bf16/tp1-pp1-${run_id}/log-gpt3-5b-2k-bf16.log` in **master pod**. The outputs like below some lines showing throughput and loss statistics every log step. The outputs like below some lines showing throughput and loss statistics every log step.

    ```plain
    Epoch 0: :  45%|████▍     | 57/128 [1:12:51<1:30:44, reduced_train_loss=6.820, global_step=56.00, consumed_samples=1.17e+5, train_step_timing in s=74.40]
    Epoch 0: :  45%|████▌     | 58/128 [1:14:05<1:29:25, reduced_train_loss=6.820, global_step=56.00, consumed_samples=1.19e+5, train_step_timing in s=74.40]
    Epoch 0: :  46%|████▌     | 59/128 [1:15:19<1:28:05, reduced_train_loss=6.820, global_step=58.00, consumed_samples=1.21e+5, train_step_timing in s=74.40]
    Epoch 0: :  47%|████▋     | 60/128 [1:16:34<1:26:46, reduced_train_loss=6.720, global_step=59.00, consumed_samples=1.23e+5, train_step_timing in s=74.40]
    ```


### Megatron-LM GPT Pretraining Guide
#### GPT Pretraining Scripts
##### GPT Pretraining Python Script
The GPT pretraining python script is based Megatron-Core and adapted from the Megatron-LM library [pretrain_gpt.py](https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_gpt.py). 

The script is available at container path `$DEEP_LEARNING_EXAMPLES_DIR/thirdparty/Megatron-LM/pretrain_gpt.py`

##### GPT Pretraining Shell Script
The GPT training Lancher shell script runs the above python script with following training parameters default (for `gpt3_5b_2k_bf16` model):

+ The number of gradient accumulation microsteps is 128, with micro batch size of 4.
+ The tensor parallelism degree is 1.
+ The pipeline parallel degree is 1.

The script is available at [`$DEEP_LEARNING_EXAMPLES_DIR/training/llm/megatron/llm/meg_lm_gpt3_5k_2k_bf16/run_meg_lm_gpt3_5k_2k_bf16.sh`](https://github.com/sallylxl/deep_learning_examples/blob/master/training/Megatron-LM/llm/gpt3/run_meg_lm_gpt3_5b_2k_bf16.sh)


#### Launch Job Using PyTorchjob Operator
##### K8S Job Lancher Shell Script

The GPT training Lancher shell script runs the above Pretraining Shell Script with following training parameters default (for `meg_lm_gpt3_5b_2k_bf16` model)  by lanching pytorchjob::

+ The number of gradient accumulation microsteps is 2048, with micro batch size of 1.
+ The tensor parallelism degree is 1.
+ The pipeline parallel degree is 1.
+ The number of workers is 1, each worker takes the resources of an entire node, which includes 8 GPUs.

The launch script `launch_meg_lm_gpt3_5b_2k_bf16.sh` is available at [deep_learning_examples/launcher_scripts/k8s/training/megatron-lm/llm/launch_meg_lm_gpt3_5b_2k_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/megatron-lm/llm/launch_meg_lm_gpt3_5b_2k_bf16.sh).

The `PyTorchJob` Kubernetes resource is defined in [deep_learning_examples/launcher_scripts/k8s/megatrion-lm/llm/pytorchjob.yaml.template](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/nemo/llm/pytorchjob-nemo.yaml.template).


##### Step-by-Step Guide

- **Update Variables**
: Edit the example running script, taking `meg_lm_gpt3_5b_2k_bf16` pretraining for example. Modify the following variables in [launch_meg_lm_gpt3_5b_2k_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/megatron-lm/llm/launch_meg_lm_gpt3_5b_2k_bf16.sh):
   + `DEEP_LEARNING_EXAMPLES_DIR`: Path to this repository (default: `/workspace/deep_learning_examples`).
   + `MOCK_DATA`: wheather use synthetic data to pretrain.  If use true data, update the following variable:
       - `DATA_DIR`: update your `DATA_DIR` to point to the pre-processed data (default: `/datasets/preset/bigscience/oscar-en`)
   + `BASE_RESULT_DIR`: Directory for experiment results (default: `$DEEP_LEARNING_EXAMPLES_DIR/results`).
   + `GPU_NUMS`: Number of GPUs used for distributed training.

- **Lanuch Command**
: 
    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-lm/llm
    ./launch_meg_lm_gpt3_5b_2k_bf16.sh
    ```

- **Scaling Up**
: If you want to scale up the number of GPUs used for distributed training, all you would need to do is set the number of `GPU_NUMS`，for example：

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/nemo/llm
    GPU_NUMS=256 ./launch_meg_lm_gpt3_5b_2k_bf16.sh
    ```

- **Monitor Training**
: You can monitor the job's progress by checking the logs of **the last RANK pod** or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/meg_lm_gpt3_5b_2k_bf16/tp1-pp1-${run_id}/log-meg_lm_gpt3_5b_2k_bf16.log` in **the last RANK pod**. The outputs like below some lines showing throughput and loss statistics every log step.


    ```plain
    [2024-09-24 07:00:40] iteration       30/     128 | consumed samples:         3840 | elapsed time per iteration (ms): 2060.4 | throughput per GPU (TFLOP/s/GPU): 502.7 | learning rate: 6.976744E-06 | global batch size:   128 | lm loss: 4.557616E+00 | loss scale: 1.0 | grad norm: 51.111 | number of skipped iterations:   0 | number of nan iterations:   0 |
    ```

### Megatron-DeepSpeed GPT Pretraining Guide
#### GPT Pretraining Scripts
##### GPT Pretraining Python Script
The GPT pretraining python script is adapted from the Megatron-DeepSpeed library [pretrain_gpt.py](https://github.com/microsoft/Megatron-DeepSpeed/blob/main/pretrain_gpt.py). 

The script is available at container path `$DEEP_LEARNING_EXAMPLES_DIR/thirdparty/Megatron-DeepSpeed/pretrain_gpt.py`

##### GPT Pretraining Shell Script
The GPT training Lancher shell script runs the above python script with following training parameters default (for `gpt3_5b_2k_bf16` model):

+ The number of gradient accumulation microsteps is 128, with micro batch size of 4.
+ The tensor parallelism degree is 1.
+ The pipeline parallel degree is 1.

The script is available at [`$DEEP_LEARNING_EXAMPLES_DIR/training/llm/Megtron-LM/llm/meg_lm_gpt3_5k_2k_bf16/run_ds_gpt3_5k_2k_bf16.sh`](https://github.com/sallylxl/deep_learning_examples/blob/master/training/Megatron-DeepSpeed/llm/gpt3/run_ds_gpt3_5b_2k_bf16.sh)


#### Launch Job Using PyTorchjob Operator
##### K8S Job Lancher Shell Script

The GPT training Lancher shell script runs the above Pretraining Shell Script with following training parameters default (for `ds_gpt3_5b_2k_bf16` model)  by lanching pytorchjob::

+ The number of gradient accumulation microsteps is 128, with micro batch size of 4.
+ The tensor parallelism degree is 1.
+ The pipeline parallel degree is 1.
+ The number of workers is 1, each worker takes the resources of an entire node, which includes 8 GPUs.

The launch script `launch_ds_gpt3_5b_2k_bf16.sh` is available at [deep_learning_examples/launcher_scripts/k8s/training/megatron-deepspeed/llm/launch_ds_gpt3_5b_2k_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/megatron-deepspeed/llm/launch_ds_gpt3_5b_2k_bf16.sh).


##### Step-by-Step Guide

- **Update Variables**
: Edit the example running script, taking `ds_gpt3_5b_2k_bf16` pretraining for example. Modify the following variables in [launch_ds_gpt3_5b_2k_bf16.sh](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/megatron-deepspeed/llm/launch_ds_gpt3_5b_2k_bf16.sh):
   + `DEEP_LEARNING_EXAMPLES_DIR`: Path to this repository (default: `/workspace/deep_learning_examples`).
   + `DATA_DIR`: update your `DATA_DIR` to point to the pre-processed data (default: `/datasets/preset/bigscience/oscar-en`)
   + `BASE_RESULT_DIR`: Directory for experiment results (default: `$DEEP_LEARNING_EXAMPLES_DIR/results`).
   + `GPU_NUMS`: Number of GPUs used for distributed training.

- **Lanuch Command**
: 

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-deepspeed/llm
    ./launch_ds_gpt3_5b_2k_bf16.sh
    ```

- **Scaling Up**
: If you want to scale up the number of GPUs used for distributed training, all you would need to do is set the number of `GPU_NUMS`，for example：

    ```plain
    cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-deepspeed/llm
    GPU_NUMS=256 ./launch_ds_gpt3_5b_2k_bf16.sh
    ```

- **Monitor Training**
: You can monitor the job's progress by checking the logs of **the last RANK pod** or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/ds_gpt3_5b_2k_bf16/tp1-pp1-${run_id}/log-ds_gpt3_5b_2k_bf16.log` in **the last RANK pod**. The outputs like below some lines showing throughput and loss statistics every log step.


    ```plain
    [2024-09-24 07:35:54,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.3953488372093024e-05, 1.3953488372093024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
    steps: 60 loss: 7.7308 iter time (s): 3.520 samples/sec: 36.364
    iteration       60/     128 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 3522.6 | learning rate: 1.395E-05 | global batch size:   128 | lm loss: 7.911283E+00 | loss scale: 1.0 | grad norm: 2.081 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 36.336 | tokens per gpu per second (tgs): 9302.141 | TFLOPs: 404.87 |
    ```

