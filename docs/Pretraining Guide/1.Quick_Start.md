---
layout: default
title: Quick Start
nav_order: 1
---

# Quick Start
## Prerequisites
### [deep_learning_examples repo](https://github.com/sallylxl/deep_learning_examples) 
Throughout the rest of this document, referenced files may be found in [deep_learning_example](https://github.com/sallylxl/deep_learning_examples) repo.

#### Key Folders

- [NeMo LLM Pretraining Scripts](https://github.com/sallylxl/deep_learning_examples/tree/master/training/nemo/llm)
: Contains example scripts for pretraining LLM Models using the [NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/).  These scripts are adapted from [NeMo-Framework-Launcher](https://github.com/NVIDIA/NeMo-Framework-Launcher/tree/main)
- [Megatron-LM LLM Pretraining Scripts](https://github.com/sallylxl/deep_learning_examples/tree/master/training/Megatron-LM/llm/gpt3)
: Contains example scripts for pretraining LLM Models that adapted from [Megatron-LM](https://github.com/NVIDIA/Megatron-LM).
- [Kubernetes Launcher Scripts](https://github.com/sallylxl/deep_learning_examples/tree/master/launcher_scripts/k8s/training)
: Includes [Pytorchjob](https://github.com/kubeflow/pytorch-operator) YAML files for Kubernetes resources and launcher scripts. Key specs of Pytorchjob YAML include:

  + Within this resource definition, there are two important `specs`: the `master` and the `worker`. the `master` and `worker` containers all run the same script using the same arguments. Both containers take the resources of an entire node, which includes 8 GPUs.
  + The PyTorch Job will also set up all the environment variables that are needed by `torchrun` and `dist` to set up distributed training, excluding `NODE_RANK`, which should be set using `RANK`. `MASTER_ADDR` and `MASTER_PORT` will point at the pod defined by the master spec.


### Dataset

#### Synthetic Data

For testing performance, use synthetic data for pretrainning based NeMo and Megatron-LM


### Docker Image
Use either:
- ScitiX NeMo container: `registry-ap-southeast.scitix.ai/hpc/nemo:24.07`
- the NGC NeMo container: `nemo:24.07`. If using NGC, clone this repository into the container or a shared storage accessible by distributed worker containers

## NeMo GPT3 Pretraining

### Lanuch Command

Start a preraining job by applying the predefined `gpt3-5b-2k-bf16` PyTorchjob YAML:

```bash
cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/nemo/llm
kubectl apply -f gpt3-5b-2k-bf16-tp1-pp1-gbs128-ckpt0-n8-pytorchjob.yaml
```

**Note**:
If you want to scale up the number of GPUs used for distributed training, you can set the number of `Worker` in the pytorchjob yaml

### Monitor Training

You can monitor the job's progress by checking the logs of the master pod or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/gpt3_5b_2k_bf16/tp1_pp1_cp1_n8_gbs128_mbs4_0/log_gpt3_5b_2k_bf16_0.out` in master pod. The outputs like below some lines showing throughput and loss statistics every log step.


```plain
Epoch 0: :  22%|██▏       | 28/128 [01:20<04:46, reduced_train_loss=5.060, global_step=27.00, consumed_samples=3584.0, train_step_timing in s=2.050]
```

### Additional Predefined Models

- [gpt3-175b-2k-bf16-tp4-pp8-gbs128-ckpt0-n128](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/nemo/llm/gpt3-175b-2k-bf16-tp4-pp8-gbs2048-ckpt0-n128-pytorchjob.yaml)

## NeMo Llama2 Pretraining

### Lanuch Command

Start a preraining job by applying the predefined `llama2-13b-bf16` PyTorchjob YAML:

```bash
cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/nemo/llm
kubectl apply -f llama2-13b-bf16-tp2-pp1-gbs128-ckpt0-n8-pytorchjob..yaml
```

**Note**:
If you want to scale up the number of GPUs used for distributed training, you can set the number of `Worker` in the pytorchjob yaml

### Monitor Training

You can monitor the job's progress by checking the logs of the master pod or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/llama2_13b_bf16/tp2_pp1_n8_gbs128_mbs1_0/log_llama2_13b_bf16_0.out` in master pod. The outputs like below some lines showing throughput and loss statistics every log step.


```plain
Epoch 0: :  12%|█▏        | 15/128 [03:18<24:52, reduced_train_loss=0.475, global_step=14.00, consumed_samples=1920.0, train_step_timing in s=12.00]
```

### Additional Predefined Models

- [llama2-70b-bf16-tp4-pp4-gbs128-ckpt0-n32](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/nemo/llm/llama2-70b-bf16-tp4-pp4-gbs128-ckpt0-n32-pytorchjob.yaml)

## NeMo nemotron-340b Pretraining

### Lanuch Command

Start a preraining job by applying the predefined `nemotron-340b` PyTorchjob YAML:

```bash
cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/nemo/llm
kubectl apply -f nemotron-340b-bf16-tp8-pp12-gbs2304-ckpt0-n96-pytorchjob.yaml
```

**Note**:
If you want to scale up the number of GPUs used for distributed training, you can set the number of `Worker` in the pytorchjob yaml

### Monitor Training

You can monitor the job's progress by checking the logs of the master pod or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/nemotron_340b_bf16/tp8_pp12_n128_gbs2304_mbs1_0/log_nemotron_340b_bf16_0.out` in master pod. The outputs like below some lines showing throughput and loss statistics every log step.


```plain
Epoch 0: :   3%|▎         | 3/110 [24:38<14:38:42, reduced_train_loss=8.980, global_step=2.000, consumed_samples=6912.0, train_step_timing in s=493.0]
```

## Megatron-LM GPT3 Pretraining

### Lanuch Command

Start a preraining job by applying the predefined `meg-lm-gpt3-5b-2k` PyTorchjob YAML:

```bash
cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-lm/llm
kubectl apply -f meg-lm-gpt3-5b-2k-bf16-tp1-pp1-gbs128-ckpt0-n8-pytorchjob.yaml
```

**Note**:
If you want to scale up the number of GPUs used for distributed training, you can set the number of `Worker` in the pytorchjob yaml

### Monitor Training

You can monitor the job's progress by checking the logs of the last rank pod or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/meg-lm-gpt3-5b-2k-bf16/tp1-pp1-0/log-meg-lm-gpt3-5b-2k-bf16.log` in last rank pod. The outputs like below some lines showing throughput and loss statistics every log step.


```plain
 [2024-09-24 07:00:40] iteration       30/     128 | consumed samples:         3840 | elapsed time per iteration (ms): 2060.4 | throughput per GPU (TFLOP/s/GPU): 502.7 | learning rate: 6.976744E-06 | global batch size:   128 | lm loss: 4.557616E+00 | loss scale: 1.0 | grad norm: 51.111 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2024-09-24 07:01:01] iteration       40/     128 | consumed samples:         5120 | elapsed time per iteration (ms): 2059.7 | throughput per GPU (TFLOP/s/GPU): 502.9 | learning rate: 9.302326E-06 | global batch size:   128 | lm loss: 2.938004E+00 | loss scale: 1.0 | grad norm: 52.166 | number of skipped iterations:   0 | number of nan iterations:   0 |
```

### Additional Predefined Models

- [meg-lm-gpt3-175b-2k-bf16-tp4-pp8-gbs2048-ckpt0-n128](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/megatron-lm/llm/meg-lm-gpt3-175b-2k-bf16-tp4-pp8-gbs2048-ckpt0-n128-pytorchjob.yaml)

## Megatron-LM Llama2 Pretraining 

### Lanuch Command

Start a preraining job by applying the predefined `llama2-13b-bf16` PyTorchjob YAML:

```bash
cd ${DEEP_LEARNING_EXAMPLES_DIR}/launcher_scripts/k8s/training/megatron-lm/llm
kubectl apply -f meg-lm-llama2-13b-bf16-tp2-pp1-gbs256-ckpt0-n16-pytorchjob.yaml
```

**Note**:
If you want to scale up the number of GPUs used for distributed training, you can set the number of `Worker` in the pytorchjob yaml

### Monitor Training

You can monitor the job's progress by checking the logs of the last rank pod or log file `${DEEP_LEARNING_EXAMPLES_DIR}/results/meg_lm_llama2_13b_bf16/tp2_pp1_n16_gbs256_mbs1_0/log_meg_lm_llama2_13b_bf16.out` in the last rank pod . The outputs like below some lines showing throughput and loss statistics every log step.


```plain
 [2024-09-24 06:47:12] iteration       50/     128 | consumed samples:        12800 | elapsed time per iteration (ms): 12650.4 | throughput per GPU (TFLOP/s/GPU): 444.2 | learning rate: 3.488372E-05 | global batch size:   256 | lm loss: 7.581806E-02 | loss scale: 1.0 | grad norm: 2.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2024-09-24 06:49:18] iteration       60/     128 | consumed samples:        15360 | elapsed time per iteration (ms): 12669.7 | throughput per GPU (TFLOP/s/GPU): 443.5 | learning rate: 4.186047E-05 | global batch size:   256 | lm loss: 1.570670E-01 | loss scale: 1.0 | grad norm: 4.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
```

### Additional Predefined Models

- [llama2-70b-bf16-tp4-pp4-gbs128-ckpt0-n32](https://github.com/sallylxl/deep_learning_examples/blob/master/launcher_scripts/k8s/training/megatron-lm/llm/meg-lm-llama2-70b-bf16-tp4-pp4-gbs512-ckpt0-n32-pytorchjob.yaml)
